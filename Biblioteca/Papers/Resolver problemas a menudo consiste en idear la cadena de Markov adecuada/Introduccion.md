---
title: Introduccion
anterior: "[[Biblioteca/Papers/Resolver problemas a menudo consiste en idear la cadena de Markov adecuada/__index|Resumen]]"
siguiente:
---
Ha pasado un siglo desde la introducción por A. A. Markov de lo que ahora conocemos como cadenas de Markov; véase Markov (1906) y Basharian *et al*. (2004). Durante este período, las cadenas de Markov han resultado no solo ser una rica fuente de bellas matemáticas, sino también inmensamente útiles en una variedad de áreas aplicadas, como la mecánica estadística, la teoría de colas, la teoría de la información, la estadística, el reconocimiento de voz y la bioinformática, por nombrar solo algunas. La forma más común de usar cadenas de Markov en estas y otras áreas es como ingredientes en la modelización de algún tipo de dinámica temporal.

Un uso completamente diferente de las cadenas de Markov es el llamado método de Monte Carlo de cadena de Markov (MCMC), pionero de Metropolis *et al*. (1953), Hastings (1970), Geman & Geman (1984) y otros. Aquí, las cadenas de Markov se aplican a situaciones que en sí mismas no necesitan involucrar ninguna dinámica temporal. El problema consiste en generar muestras computacionales con alguna distribución prescrita pero típicamente muy complicada $\pi$ sobre un espacio de estados grande $S$, y la idea del método MCMC es que, en situaciones donde parece prácticamente imposible muestrear directamente de $\pi$, puede ser fácil muestrear desde el núcleo de transición de alguna cadena de Markov irreducible y aperiódica $X = \{X(0), X(1), \dots\}$ sobre $S$ cuya distribución estacionaria única es precisamente $\pi$.

Si la cadena tiene la propiedad de convergencia rápida a la estacionariedad (como se espera), entonces una manera sencilla de generar un objeto aleatorio con valores en $S$ cuya distribución esté cerca de $\pi$ es iniciar la cadena con $X(0)$ elegido arbitrariamente, ejecutarla por un tiempo $n$ y tomar $X(n)$ como muestra. Véase, por ejemplo, Gilks *et a*l. (1996) o Häggström (2002) para introducciones a la teoría y práctica del MCMC.

El propósito del presente artículo es elaborar sobre la idea menos conocida de que el ingrediente central del método MCMC – es decir, la introducción de una cadena de Markov diseñada para tener una distribución estacionaria prescrita $\pi$ – es útil en una variedad de contextos que no involucran simulaciones computacionales. En las aplicaciones que tengo en mente, no es necesario implementar ni ejecutar las cadenas de Markov: basta con pensarlas a un nivel más abstracto.

Todo matemático necesita tener un conjunto de herramientas y trucos para usar en diversas situaciones, y espero convencer a los lectores de que la disposición a probar ideas basadas en cadenas de Markov es un recurso lo suficientemente útil como para querer incluirlo en su propio caja de herramientas.

En este punto, parece apropiada una línea de la influyente introducción de Lindvall (1992) sobre los métodos de acoplamiento: “Conocer un método es haber aprendido cómo funciona. Lo que tenemos por delante es una colección de aplicaciones de unas pocas ideas básicas” (p. 6). En las secciones siguientes, me centraré en tres ejemplos fundamentales. En la sección 2, discutiré el uso de cadenas de Markov para probar la muy útil desigualdad de correlación de Harris (1960). Luego, continuaré con ejemplos de mi propia práctica: una desigualdad de dominación necesaria en un problema surgido en muestreo de encuestas en la sección 3, y una desigualdad de correlación condicional para modelos de percolación en la sección 4.

Un aspecto de mi enfoque *“lindvalliano”* en este artículo es que no pretendo ofrecer una revisión exhaustiva del tema. Para un subtema particular que queda fuera de la discusión pero que recomiendo al lector ambicioso, menciono la explotación de ideas del enfoque *"coupling-from-the-past"* de Propp & Wilson (1996) en el llamado *"perfect MCMC"*, para el análisis riguroso de las propiedades ergódicas de campos aleatorios gibbsianos; esta idea fue concebida por primera vez por van den Berg & Steif (1998) y luego explotada más a fondo por Häggström & Steif (2000) y Häggström *et al*. (2000, 2002).